---
title: "main"
author: "Yue Cao"
output: html_document
---

```{r}
library(rvest)
library(stringr)
library(dplyr)
library(ggplot2)

```

```{r}
url <- "https://stackoverflow.com/jobs?sort=i&q=Data+Science&pg=1"
### if you want urls for all the pages
# urls <- paste0("https://stackoverflow.com/jobs?sort=i&q=Data+Science&pg=", 1:7)
fields <- url %>% read_html() %>% html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "-job-item", " " ))]')


urls <- paste0("https://stackoverflow.com/jobs?sort=i&q=Data+Science&pg=", 1:7)
for(i in seq_along(urls)) {
  url <- urls[i]
  fields <- url %>% read_html() %>% html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "-job-item", " " ))]')
  job.urls <- paste0("https://stackoverflow.com",
                     unname(unlist(sapply(x, function(x) if(x["class"]=="job-link") x["href"]))))

  

  
  link_clean<-joblink[which(is.na(joblink)==FALSE)]
  title_clean<-title[which(is.na(joblink)==FALSE)]
  company_clean<-company[which(is.na(joblink)==FALSE)]
  location_clean<-location[which(is.na(joblink)==FALSE)]
  company_clean<-str_replace_all(company_clean, "[\r\n|\n|\t|\r|,|/|<|>|\\.]|cmp", '')
  
  CombineInfo<-cbind(title_clean,company_clean,location_clean,link_clean)
  BasicInfo<-rbind(BasicInfo,CombineInfo)
  Sys.sleep(0.1)
  Sys.sleep(3)
}


KEYWORDS <- c('Hadoop','Python','\\bSQL', 'NoSQL','\\bR\\b', 'Spark', 'SAS', 'Excel', 'AWS', 'Azure', 'Java', 'Tableau')






```