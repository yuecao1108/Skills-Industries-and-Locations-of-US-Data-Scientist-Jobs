---
title: "main"
author: "Yue Cao"
output: html_document
---

```{r}
pkgs= c("rvest","stringr", "dplyr","ggplot2", "knitr", "dendextend", "xtable", "cowplot")
not_installed <- pkgs[!pkgs %in% installed.packages()]
if (length(not_installed) > 0) {
  install.packages(not_installed, repos = "https://cloud.r-project.org/")
}
library(rvest)
library(stringr)
library(dplyr)
library(ggplot2)
library(knitr)
library(dendextend)
library(xtable)
library(cowplot)

```

  
1. Data from stackoverflow.com
```{r, eval=FALSE}
## stackoverflow  
## partial codes are credit to Stephen Cristiano, jobs_scrape.R, (https://slack-files.com/T6TTWE3G8-F75420VQD-7f4696035e)


urls <- paste0("https://stackoverflow.com/jobs?sort=i&q=Data+Science&pg=", 1:8)
skill <- list()
skilltag <- rep(NA,5000)
company <- rep(NA,5000)
location <- rep(NA,5000)
industry <- rep(NA,5000)
jobtitle <- rep(NA, 5000)
n <- 0 # count the number of jobs
keywords <- c('\\bHadoop\\b', '\\bSpark\\b', 
              '\\bR\\b', '\\bSAS\\b', '\\bStata\\b', 
              '\\bJava\\b', '\\bPerl\\b', '\\bPython\\b',
              '\\bSQL\\b', '\\bNoSQL\\b','\\bTableau\\b', '\\bExcel\\b',
              'Machine Learning','Amazon Web Service' )
keywords_display = unlist(lapply(keywords, function(x) gsub("[\\]b", "", x)))

get_keyword = data.frame()

for(i in seq_along(urls)) {
  url <- urls[i]
  fields <- url %>% read_html() %>% html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "-job-item", " " ))]')
  #session <- html_session(url)
  x <- fields %>% html_nodes(".job-link") %>% html_attrs()
  job.urls <- paste0("https://stackoverflow.com",
                     unname(unlist(sapply(x, function(x) if(x["class"]=="job-link") x["href"]))))
  job.urls <- unique(job.urls)
  #job.names <- url %>% read_html() %>% html_nodes(".g-col10 .job-link") %>% html_text()
  for ( j in job.urls){
    tryCatch({
    page <- html_session(j)
    add_company <- page %>% html_node(".employer") %>% html_text() %>% trimws()
    if(is.na(add_company) | add_company %in% company){
      next
    }else{
      n = n + 1
      skill[[n]] <- page %>% html_nodes("#overview-items .no-tag-menu") %>% html_text() %>% trimws()
      skilltag[n] <- paste(unlist(skill[[n]]),collapse = ", ")
      company[n] <- add_company
      jobtitle[n] <- page %>% html_node(".title") %>% html_text() %>% trimws() 
      loc <- page %>% html_nodes(".-description .-location") %>% html_text() %>% trimws() 
      location[n] <- ifelse(length(loc)>0, gsub('- \n', "", loc),NA)
      indus <- page %>% html_nodes("#overview-items .g-column+ .g-column .g-col:nth-child(1) .-value") %>% html_text() %>% trimws()
      industry[n] <- ifelse(length(indus)>0,indus,NA)
      description <-page %>% html_nodes(".-technologies , .-job-description") 
         if (length(description)>0){
        keyword_truefalse = sapply(keywords, function(x) any(grep(x, description, ignore.case = TRUE)))
        get_keyword = rbind(get_keyword, keyword_truefalse)}
        else{get_keyword = rbind(get_keyword, rep(NA, length(keywords)))}
        }
    }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")}) #end tryCatch
    Sys.sleep(2)
  }
  Sys.sleep(2)
  
}



# divide location as city and state
location <- location[1:n]  %>%str_split(", ") 
for(i in 1: n){
  l <- length(location[[i]])
  if(l > 2){
    location[[i]] <- location[[i]][-(1:(l-2))]
  }
}
location <- t(as.data.frame(location, stringsAsFactors = FALSE))



#output 
skillset <- as.data.frame(table(unlist(skill)))
colnames(skillset) <- c("skill","freq")

 dat <- data.frame(jobtitle = jobtitle[1:n], company = company[1:n],
                   city = location[,1], state = location[,2], industry = industry[1:n],
                   skilltag = skilltag[1:n])
# write.csv(skillset, "stack_skillset.csv", row.names = F)


colnames(get_keyword) <- keywords_display 
stack_data <- cbind(dat, get_keyword)
write.csv(stack_data, "stack_fulldata.csv",row.names = F)


```


2. Data from dice.com
```{r, eval=FALSE}
# Dice

urls <- paste0("https://www.dice.com/jobs/q-data_science-startPage-",1:40, "-jobs?searchid=9816474276503&stst=")

keywords <- c('\\bHadoop\\b', '\\bSpark\\b', 
              '\\bR\\b', '\\bSAS\\b', '\\bStata\\b', 
              '\\bJava\\b', '\\bPerl\\b', '\\bPython\\b',
              '\\bSQL\\b', '\\bNoSQL\\b','\\bTableau\\b', '\\bExcel\\b',
              'Machine Learning','Amazon Web Service' )
keywords_display = unlist(lapply(keywords, function(x) gsub("[\\]b", "", x)))
get_keyword = data.frame()
company <- rep(NA,5000)
location <- rep(NA,5000)
jobtitle <- rep(NA, 5000)
n <- 0 # count the number of jobs

for(i in seq_along(urls)) {
  url <- urls[i]
  session <- html_session(url)
  link <- session %>% html_nodes(".dice-btn-link.loggedInVisited") %>% html_attr("href")
  link <- paste0('https://www.dice.com', link)
  link <- unique(link)
  for (j in link){
    tryCatch({
      page <- html_session(j)
      add_company <- page %>% html_node(".dice-btn-link span") %>% html_text() %>% trimws()
      if(is.na(add_company) | add_company %in% company){
        next
      }else{
        n = n + 1
        company[n] <- add_company
        loc <- page %>% html_nodes(".location span") %>% html_text() %>% trimws() 
        location[n] <- ifelse(length(loc[1])>0, loc[1],NA)
        jobtitle[n] <- page %>% html_node("#jt") %>% html_text() %>% trimws() 
        description <- page %>% html_nodes("#jobdescSec") 
         if (length(description)>0){
        keyword_truefalse = sapply(keywords, function(x) any(grep(x, description, ignore.case = TRUE)))
        get_keyword = rbind(get_keyword, keyword_truefalse)}
        else{get_keyword = rbind(get_keyword, rep(NA, length(keywords)))}
        }
    }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")}) #end tryCatch
    Sys.sleep(2)
  }  
Sys.sleep(2)
}

# divide location as city and state
location <- location[1:n]  %>%str_split(", ") 
for(i in 1: n){
  l <- length(location[[i]])
  if(l > 2){
    location[[i]] <- location[[i]][-(1:(l-2))]
  }
}
location <- t(as.data.frame(location, stringsAsFactors = FALSE))

dat <- data.frame(jobtitle = jobtitle[1:n], company = company[1:n], city = location[,1], 
                  state = location[,2])

#write.csv(dat, "dice_dat.csv",row.names = F)
colnames(get_keyword) <- keywords_display 
#write.csv(get_keyword, "dice_get_keyword.csv",row.names = F)

dice_data <- cbind(dat, get_keyword)
write.csv(dice_data, "dice_fulldata.csv",row.names = F)



```


3. Data from glassdoor.com
```{r, eval=FALSE}
urls <- paste0("https://www.glassdoor.com/Job/data-science-jobs-SRCH_KO0,12_IP",1:40, ".htm?jobType=fulltime")

get_keyword = data.frame()
company <- rep(NA,5000)
location <- rep(NA,5000)
jobtitle <- rep(NA, 5000)
industry <- rep(NA, 5000)
size <- rep(NA, 5000)
n <- 0 # count the number of jobs

for(i in seq_along(urls)) {
  url <- urls[i]
  session <- html_session(url)
  link <- session %>% html_nodes(".flexbox .jobLink") %>% html_attr("href")
  link <- paste0('https://www.glassdoor.com', link)
  link <- unique(link)
  for (j in link){
    tryCatch({
      page <- html_session(j)
      add_company <- page %>% html_node(".padRtSm") %>% html_text() %>% trimws()
      if(is.na(add_company) | add_company %in% company){
        next
      }else{
        n = n + 1
        company[n] <- add_company
        loc <- page %>% html_node(".subtle") %>% html_text() %>% trimws() 
        location[n] <- ifelse(length(loc)>0, substr(loc, 4, nchar(loc)),NA)
        jobtitle[n] <- page %>% html_node("#HeroHeaderModule .strong") %>% html_text() %>% trimws() 
        read <-  j %>% readLines(warn = FALSE)
        extract_industry = grep("'sector' :", read, value = TRUE)
        industry[n] <- ifelse(length(extract_industry)>0,
                              as.character(strsplit(extract_industry,"[\"]")[[1]][2]), NA)
        extract_size <- grep("'size' :", read, value = TRUE)
        size[n] <- ifelse(length(extract_size)>0,
                          strsplit(extract_size,"[\"]")[[1]][2], NA)
        description <- page %>% html_nodes(".desc") 
        if (length(description)>0){
        keyword_truefalse = sapply(keywords, function(x) any(grep(x, description, ignore.case = TRUE)))
        get_keyword = rbind(get_keyword, keyword_truefalse)}
        else{get_keyword = rbind(get_keyword, rep(NA, length(keywords)))}
        }
    }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")}) #end tryCatch
    Sys.sleep(2)
  }  
  Sys.sleep(2)
}

# divide location as city and state
location <- location[1:n]  %>%str_split(", ") 
for(i in 1: n){
  l <- length(location[[i]])
  if(l > 2){
    location[[i]] <- location[[i]][-(1:(l-2))]
  }
}
location <- t(as.data.frame(location, stringsAsFactors = FALSE))

dat <- data.frame(jobtitle = jobtitle[1:n], company = company[1:n], 
                  industry = industry[1:n], size = size[1:n],
                  city = location[,1], state = location[,2])


#write.csv(dat, "/users/ycao/advdatasci/data/glass_dat.csv",row.names = F)
colnames(get_keyword) <- keywords_display 
#write.csv(get_keyword, "/users/ycao/advdatasci/data/glass_get_keyword.csv",row.names = F)

glass_data <- cbind(dat, get_keyword)
write.csv(glass_data, "glass_fulldata.csv",row.names = F)




```




4. Data Cleaning 
```{r}

#  stack_fulldata <- read.csv("~/Desktop/advdatascience/data/7stack_full.csv")
#  dice_fulldata <- read.csv("~/Desktop/advdatascience/data/dice/4dice_fulldata.csv")
# glass_fulldata <- read.csv("~/Desktop/advdatascience/data/glass/4glass_fulldata.csv")

stack_fulldata <- read.csv("stack_fulldata.csv")
 dice_fulldata <- read.csv("dice_fulldata.csv")
glass_fulldata <- read.csv("glass_fulldata.csv")


stack_fulldata$source <- "Stackoverflow"
stack_fulldata <- stack_fulldata[complete.cases(stack_fulldata),] #remove rows containing NA
d1 <- stack_fulldata[,-c(6)] #delete skilltag
 stack_skill = stack_fulldata %>% select(Hadoop:Amazon.Web.Service)
skill_count = apply(stack_skill, 2, sum)
skills1 = data.frame(skill = names(skill_count), count = skill_count)
skills1 = arrange(skills1,count)
skills1$percent = skills1$count / nrow(stack_skill) *100


dice_fulldata$source <-"Dice"
dice_fulldata <- dice_fulldata[complete.cases(dice_fulldata),]
d2 <- dice_fulldata
dice_skill = dice_fulldata %>% select(Hadoop:Amazon.Web.Service)
skill_count = apply(dice_skill, 2, sum)
skills2 = data.frame(skill = names(skill_count), count = skill_count)
skills2 = arrange(skills2,count)
skills2$percent = skills2$count / nrow(dice_skill) *100

glass_fulldata$source <- "Glassdoor"
glass_fulldata <- glass_fulldata[complete.cases(glass_fulldata),]
# clean size
glass_fulldata$size <- gsub("--1","+",glass_fulldata$size)
glass_fulldata <- glass_fulldata[which(glass_fulldata$size!="-1-0"),]
# clean industry
glass_fulldata$industry <- gsub("&amp;","&",glass_fulldata$industry)

d3 <- glass_fulldata
 glass_skill = glass_fulldata %>% select(Hadoop:Amazon.Web.Service)
skill_count = apply(glass_skill, 2, sum)
skills3 = data.frame(skill = names(skill_count), count = skill_count)
skills3 = arrange(skills3,count)
skills3$percent = skills3$count / nrow(glass_skill) *100


d1 <- d1[!duplicated(d1),] # remove duplicated data
d2 <- d2[!duplicated(d2),] # remove duplicated data
d3 <- d3[!duplicated(d3),] # remove duplicated data


```


5. Exploratory Data Analysis
```{r}
# compare data scientist skills on Dice, Stackoverflow and Glassdoor
skills1$source <- "Stackoverflow" 
skills2$source <- "Dice"
skills3$source <- "Glassdoor"
d_skills <- rbind(skills1,skills2,skills3)
p4 <- ggplot(d_skills,
       aes(x = reorder(skill, percent), y = percent)) + 
  geom_bar(aes(fill= source),position = "dodge", stat = "identity",width = 0.7) + 
  coord_flip() + 
  facet_grid( .~ source)+
  labs(x="Skills", y="Frequency(%)", title="Skill Frequency for Data Scientist jobs")
p4

skills1 <- arrange(skills1, skill)
skills2 <- arrange(skills2, skill)
skills3 <- arrange(skills3, skill)

totalcount <- data.frame(skill = skills1$skill, count = skills1$count+skills2$count+skills3$count)
                    
totalcount$percent = totalcount$count/sum(nrow(stack_fulldata)+nrow(glass_fulldata)+nrow(dice_fulldata)) *100
totalcount <- arrange(totalcount, desc(percent))
totalcount$percent <- paste0(round(totalcount$percent,2),"%")
tab1 <- xtable(totalcount[1:5,])
print(tab1, type="latex")


dat <- rbind(stack_fulldata[,c(1:4,7:21)],glass_fulldata[,c(1:2,5:21)],dice_fulldata) #1043 obs in total


# skill frequency
dat$location <- paste0(dat$city, ", ", dat$state)
jobcount <- dat %>% group_by(location) %>% tally()
jobcount <- jobcount %>% arrange(desc(n))
top10_jobcount <- jobcount[1:10,]
p5 <- ggplot(top10_jobcount,
       aes(x = reorder(location, n), y = n)) + 
  geom_bar(aes(fill=location),position = "dodge", stat = "identity",width = 0.7) + 
  coord_flip() + 
  #facet_grid( .~ source)+
  labs(x="Location", y="Number of jobs", title="Number of Data Scientist jobs in Different Locations") +
 theme(legend.position="none")
p5  

# industry
industrycount <- glass_fulldata %>% group_by(industry) %>% tally()
industrycount <- industrycount %>% arrange(desc(n))
top10_industrycount <- industrycount[1:10,]
p6 <- ggplot(top10_industrycount,
       aes(x = reorder(industry, n), y = n)) + 
  geom_bar(aes(fill=industry),position = "dodge", stat = "identity",width = 0.7) + 
  coord_flip() + 
  #facet_grid( .~ source)+
  labs(x="Industry", y="Number of jobs", title="Number of Data Scientist Jobs in Different Industries") + theme(legend.position="none")
p6  

```


6.  Hierarchical Clustering
```{r}
# Hierarchical Clustering on industry types and cross-validation
set.seed(6783459)
h <- glass_fulldata %>% group_by(industry) %>% summarise(n=n()) %>% filter(n>=10)
hdat <- glass_fulldata %>% filter(industry %in% h$industry)
index <- sample(1:nrow(hdat), round(nrow(hdat)/2))
train <- hdat[index,c(3,7:20)]
test <- hdat[-index,c(3,7:20)]

traindat <- aggregate(train[,-1], by=list(train$industry), FUN=sum)
trainrowname <- traindat$Group.1
rownames(traindat) <- trainrowname
train_dend <- dist(scale(traindat[,-1])) %>% hclust %>% as.dendrogram

testdat <- aggregate(test[,-1], by=list(test$industry), FUN=sum)
testrowname <- testdat$Group.1
rownames(testdat) <- testrowname
test_dend <- dist(scale(testdat[,-1])) %>% hclust %>% as.dendrogram

train_test_dend <- dendlist(train = train_dend, test = test_dend)
cor <- train_test_dend %>% cor.dendlist


#compare the results with a tanglegram

tanglegram<- train_test_dend %>% ladderize %>% untangle %>%
   set("branches_k_color", k = 4)
train_branches_colors <- get_leaves_branches_col(tanglegram$train)
tanglegram%>% tanglegram(fast = TRUE, color_lines = train_branches_colors,
                         margin_inner= 10.5, dLeaf =-0.1)


# plot
par(mar = c(5,5,5,15))
 train_dend %>% color_branches(k=4) %>% color_labels(k=4) %>% plot(horiz=T, main = "Hierarchical Clustering for Industry Types Based on Skill Needs")
# add cut line
abline(v = heights_per_k.dendrogram(train_dend)["4"] + .6, lwd = 2, lty = 2, col = "blue")


save.image(file = "jobdata.RData") #save all R objects in the workspace
```






